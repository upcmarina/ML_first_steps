# METROPOLIS ALGORITHM

## PREVIOUS CONCEPTS

The **Metropolis Algorithm** is one of the **Markov Chain Monte Carlo (MCMC)** sampling methods.

We must check some relevant concepts before going into detail:

* **MCMC METHODS**: used to approximate the posterior distribution of a parameter of interest by random sampling in a probabilistic space. The idea behind MCMC is to perform an intelligent search of the space.

* **PARAMETER OF INTEREST**: number that summarizes a phenomenon we’re interested in. In general, we use statistics to estimate parameters. For example, if we want to learn about the height of human adults, our parameter of interest might be average height in inches. 

* **DISTRIBUTION**: mathematical representation of every possible value of our parameter and how likely we are to observe each one. The most famous example is a bell curve.

* **PRIOR DISTRIBUTION**: the distribution representing our beliefs about a parameter, because it captures our beliefs prior to seeing any data.

* **LIKELIHOOD DISTRIBUTION**: summarizes what the observed data are telling us, by representing a range of parameter values accompanied by the likelihood that each each parameter explains the data we are observing. Estimating the parameter value that maximizes the likelihood distribution is just answering the question: what parameter value would make it most likely to observe the data we have observed? In the absence of prior beliefs, we might stop there.

* **POSTERIOR DISTRIBUTION**: obtained combining the prior and the likelihood distributions. This tells us which parameter values maximize the chance of observing the particular data that we did, taking into account our prior beliefs. **MCMC methods allow us to estimate the shape of a posterior distribution in case we can’t compute it directly.**

## ABOUT

Once we have revised the previous concepts, we can talk about the **Metropolis Algorithm**. 

This algorithm helps us to obtain the shapes of the probability distribution that we are trying to estimate. When you only have the Probability Density Function (pdf) of the target distribution, you can use it to draw traces of the samples.

The objective is to get the **shape**, not to find the maximum. 

We visit each point an amount of time that is proportional to its probability.

## STEPS

1. Generate a candidate **x'**. This candidate will be the one assessed for the next sample, and it is generated by taking one value from a proposed distribution.

2. Calculate acceptance ratio for **x'**. 
 
If the generated candidate implies going **uphill**, the value of the acceptance ratio will be **greater than 1**. Otherwise, if the generated candidate implies going **downhill**, the ratio will be always **smaller than 1**. This is crucial for the next step.

3. Accept or reject **x'**. Here we will generate a random number **u**, its value will be **between 0 and 1**. Then we will compare **u** and the acceptance ratio. If the acceptance ratio is greater than **u**, you accept the candidate and move to the point. Otherwise, you don't accept the candidate and start again, generating a new candidate and evaluating it.

When you are going **upphill**, you always accept the candidate, as the acceptance ratio is **greater than 1** and it will be always greater than **u** (as its maximum value is 1).

When you are going **downhill**, you only accept the candidate if the acceptance ratio is greater than the random number. The algorithm follows this strategy in order to not be trapped in certain points of the distribution. 

## BIBLIOGRAPHY

Robert, C. P. (2015). The Metropolis–Hastings Algorithm.
